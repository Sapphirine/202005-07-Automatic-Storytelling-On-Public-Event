{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story:\n",
    "    def __init__(self,keywords):\n",
    "        self.keywords=keywords\n",
    "    def collect(self):\n",
    "\n",
    "        consumer_key = \"9AZig9wq75kt0FYgjrhQE36LY\" \n",
    "\n",
    "        consumer_secret = \"uHlZ5ymsaOZ3PwDpXzMBpUPTLyVewYSQrgBoygol3BkdTypIyL\"\n",
    "\n",
    "        access_token = \"1230732258564132864-yCyObKdgwH8lEWySMzNw9CSXfmcxav\"\n",
    "\n",
    "        access_token_secret = \"ssCP3olbTGl3YWjB3zQkz3bvtPRIdiu9nMFuyavWCwgHp\"\n",
    "\n",
    "\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = tweepy.API(auth, proxy=\"127.0.0.1:1081\",wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "        #Search specific news account from twitter\n",
    "        search_results=[]\n",
    "        #= api.search(lang='en',q='coronavirus',count=100)\n",
    "        search_results.append(api.user_timeline(id='nytimes',include_rts=False,count=30,tweet_mode=\"extended\"))\n",
    "        search_results.append(api.user_timeline(id='BreakingNews',include_rts=False,count=30,tweet_mode=\"extended\"))\n",
    "        search_results.append(api.user_timeline(id='cnnbrk',include_rts=False,count=30,tweet_mode=\"extended\"))\n",
    "        search_results.append(api.user_timeline(id='WSJbreakingnews',include_rts=False,count=20,tweet_mode=\"extended\"))\n",
    "        search_results.append(api.user_timeline(id='ABCNewsLive',include_rts=False,count=20,tweet_mode=\"extended\"))\n",
    "        search_results.append(api.user_timeline(id='SkyNewsBreak',include_rts=False,count=20,tweet_mode=\"extended\"))\n",
    "        search_results.append(api.user_timeline(id='TWCBreaking',include_rts=False,count=20,tweet_mode=\"extended\"))\n",
    "        self.pic=[] #get pictures\n",
    "        \n",
    "        tweetlist=[]\n",
    "        for account in search_results:\n",
    "            for tweet in account:\n",
    "                if self.keywords in tweet._json['full_text']:\n",
    "                    tweetlist.append([tweet._json['full_text'].split('http')[0],tweet._json['created_at'].split(' ')[0:3]])\n",
    "                    media=tweet.entities.get('media', [])\n",
    "                    if(len(media) > 0):\n",
    "                          self.pic.append(media[0]['media_url'])\n",
    "        self.text=[] #get text\n",
    "\n",
    "        self.timeline=[] #get timeline\n",
    "        for t in tweetlist:\n",
    "            txt=t[0].split(':')[-1]\n",
    "\n",
    "            if len(txt.split())>=10:\n",
    "                self.text.append(t[0])\n",
    "                self.timeline.append(t[1])\n",
    "        return (self.text,self.timeline,self.pic)\n",
    "    def process(self):\n",
    "        text1=[]\n",
    "        for t in self.text:\n",
    "            t=re.sub(r'[\\s]', ' ', t)\n",
    "            if t[-1:]==' ':\n",
    "                t=t[:-1]\n",
    "            if t[-1:]=='.':\n",
    "                t=t[:-1]\n",
    "            text1.append(t)\n",
    "        contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        def text_cleaner(text):\n",
    "            newString = text.lower()#小写\n",
    "            newString = BeautifulSoup(newString, \"lxml\").text\n",
    "            newString = re.sub('\"','', newString)\n",
    "            newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])#contraction\n",
    "            newString = re.sub(r\"'s\\b\",\"\",newString) #去掉’s\n",
    "            newString = re.sub(\"[^\\.\\%0-9a-zA-Z]\", \" \", newString)\n",
    "            newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "            tokens=newString.split()\n",
    "            return (\" \".join(tokens)).strip()\n",
    "\n",
    "        self.cleaned_text = [] \n",
    "        for t in text1:\n",
    "            self.cleaned_text.append(text_cleaner(t))\n",
    "    def embedding(self):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "        vectorizer = CountVectorizer()  \n",
    " \n",
    "        X = vectorizer.fit_transform(self.cleaned_text)  \n",
    " \n",
    "        word = vectorizer.get_feature_names()  \n",
    "        self.text_vec=X.toarray()\n",
    "    def cluster(self,n_cluster=3):\n",
    "        from Bio.Cluster import kcluster\n",
    "        clusterid, error, nfound = kcluster(self.text_vec, n_cluster, dist='u',npass=100)\n",
    "        self.text_afterclustering={}\n",
    "        for i in range(0,len(clusterid)):\n",
    "            if clusterid[i] in self.text_afterclustering:\n",
    "                self.text_afterclustering[clusterid[i]].append(i)\n",
    "            else:\n",
    "                self.text_afterclustering[clusterid[i]]=[i]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords='coronavirus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "story=Story('coronavirus')\n",
    "story.collect()\n",
    "story.process()\n",
    "story.embedding()\n",
    "story.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_afterclustering=story.text_afterclustering\n",
    "text=story.text\n",
    "timeline=story.timeline\n",
    "pic=story.pic\n",
    "cleaned_text=story.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 4, 5, 8, 9, 10, 12, 13, 15, 18, 23, 27],\n",
       " 2: [1, 2, 11, 22],\n",
       " 1: [3, 6, 7, 14, 16, 17, 19, 20, 21, 24, 25, 26]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_afterclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://pbs.twimg.com/media/EYGluzkWoAE7jcf.jpg',\n",
       " 'http://pbs.twimg.com/media/EYExmWTXYAAuf5W.jpg',\n",
       " 'http://pbs.twimg.com/media/EYEUJGxWkAEMXUi.jpg',\n",
       " 'http://pbs.twimg.com/media/EX_YzVsXQAIk142.jpg',\n",
       " 'http://pbs.twimg.com/amplify_video_thumb/1261085480902430721/img/CIFrrKHS6nXFgpr7.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic0](http://pbs.twimg.com/media/EYGluzkWoAE7jcf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%we get 2 lists:\n",
    "import time\n",
    "from datetime import datetime\n",
    "times=[]\n",
    "for i in timeline:\n",
    "    t='2020 '+' '.join(i)\n",
    "    t=datetime.strptime(t,'%Y %a %b %d')\n",
    "    times.append(t)\n",
    "def sortby_timeline(idlist):\n",
    "       return (sorted(idlist,key=lambda i:times[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printstory1(sortedidlist):\n",
    "    story=''\n",
    "    for i in sortedidlist:\n",
    "        story+=cleaned_text[i]+'. '\n",
    "    return story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printstory2(sortedidlist):\n",
    "    story=''\n",
    "    #先总结\n",
    "    connectword=['On the same day, ','Also, ',\"What's more, \",'While, ','furthermore, ','then, ', 'besides, ']\n",
    "    former_time=None\n",
    "    for i in sortedidlist:\n",
    "        current_time=' '.join(timeline[i][1:3])\n",
    "        if current_time==former_time:\n",
    "            n=random.randint(0,6)\n",
    "            prefix=connectword[n]\n",
    "        else:\n",
    "            prefix='On '+current_time+', '\n",
    "        former_time=current_time\n",
    "        sen=prefix+cleaned_text[i]+'. '\n",
    "        story+=sen\n",
    "    return(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "storytelling=[]\n",
    "for i in range(3):\n",
    "    idlist=text_afterclustering[i]\n",
    "    sortedidlist=sortby_timeline(idlist)\n",
    "    smallstory=printstory1(sortedidlist)\n",
    "    cls=classify(smallstory)\n",
    "    storytelling.append('Focus on '+cls+' information about '+keywords+'. '+printstory2(sortedidlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus on life information about coronavirus. On May 08, vp press sec. katie miller tests positive for coronavirus she tells nbcnews saying she is asymptomatic. On May 09, president trump announces the federal government will purchase 3b of dairy meat and produce from farmers as the coronavirus pandemic continues to disrupt the food supply chain. On May 10, uk pm johnson unveils roadmap for getting britain out of the coronavirus lockdown outlining a series of staggered steps that he says would be conditional on how diligently the public follows the government advice. On May 12, live dr. fauci and other top health officials testify at us senate coronavirus hearing. On May 13, india pm modi says 266b will be provided to boost the country economy amid the coronavirus pandemic. furthermore, prime minister boris johnson has paid tribute to railway ticket officer worker belly mujinga who died with coronavirus after being spat at while on duty saying the fact that she was abused for doing her job is utterly appalling. On May 14, the un is forecasting that the coronavirus pandemic will shrink the world economy by 3.2% this year the sharpest contraction since the great depression. On the same day, wisconsin supreme court strikes down extension of stay at home orders during the coronavirus pandemic as unlawful invalid and unenforceable after finding the state health commissioner exceeded authority. While, the pentagon has removed its lead official responsible for executing the defense production act to increase production of key equipment to combat coronavirus. On May 15, breaking news j.c. penney filed for bankruptcy. the 118 year old department store is by far the biggest retail casualty of the coronavirus pandemic. furthermore, the chinese novelist fang fang lives in downtown wuhan the epicenter of the coronavirus outbreak. after the city went into quarantine she began keeping a diary about her experience. for nytimesbooks dwightgarner reviews wuhan diary.. furthermore, the fda halted a coronavirus testing program in the seattle area that was backed by bill gates and local health officials. washington state authorized it but the program was told that it now needs approval directly from the u.s. government. \n",
      "Focus on life information about coronavirus. On May 09, 3 children have now died in new york from an inflammatory syndrome believed to be related to the coronavirus gov. cuomo says. besides, russia recorded 10 817 new coronavirus cases on saturday according to health officials topping 10 000 cases for the 7th day in a row. On May 10, more than 4m confirmed cases of coronavirus have been reported around the world according to the latest johns hopkins university data. the global death toll has reached nearly 280 000. On May 14, fbi seizes sen. burr cell phone in probe over stock sales after coronavirus briefing. On the same day, ousted vaccine director rick bright says there appears to be no master coordinated plan on how the us is responding to coronavirus. On May 15, most u.s. states have reopened partially reopened or plan to open in the next week even as new cases of the coronavirus continue to grow around the country. only 4 states d.c. and puerto rico remain largely shut down. here is where each state stands. then, president trump has named former pharmaceutical executive moncef slaoui and army gen. gustave perna to lead operation warp speed the effort to develop a coronavirus vaccine. What's more, us retail sales fell a record 16.4% in april underscoring just how badly retailers are struggling during the coronavirus crisis. then, breaking medical professionals are investigating more than 200 confirmed or suspected cases of a mysterious illness thought to be linked to the coronavirus and affecting children according to a new abc news survey. besides, . willcarr has more on a usda initiative that is getting millions of meals to rural children during the novel coronavirus pandemic. On May 16, house passes 3t coronavirus heroes relief package in 208 to 199 vote bill now moves to republican led senate. president trump this week called the proposal doa.. Also, the house passes a 3 trillion coronavirus relief bill despite opposition from republicans and some democrats. the senate is not expected to take it up. \n",
      "Focus on tech information about coronavirus. On May 13, paul manafort has been early released from prison amid coronavirus concerns had been sentenced to 7.5 years in prison. On May 14, some countries are seeing a spike in known coronavirus deaths. others are struggling to find a path to normalcy. On May 15, economic pressures are fueling reopenings in the u.s. even as coronavirus cases continue to climb. businesses that are opening up are doing so under restrictions such as allowing fewer customers and requiring workers and customers to wear masks. then, we are also tracking which states are or are not flattening the curve. as of friday arizona arkansas and south dakota are still seeing increases in coronavirus cases. 28 others are holding steady at current rates. 19 states are showing decreases. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def wholestory(story):\n",
    "    ws=''\n",
    "    for smallstory in story:\n",
    "        ws+=smallstory+'\\n'\n",
    "    return(ws)\n",
    "print(wholestory(storytelling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(corpus):\n",
    "    idfs = {}\n",
    "    d = 0.0\n",
    "    # 统计词出现次数\n",
    "    for doc in corpus:\n",
    "        d += 1\n",
    "        counted = []\n",
    "        for word in doc.split():\n",
    "            if not word in counted:\n",
    "                counted.append(word)\n",
    "                if word in idfs: \n",
    "                    idfs[word] += 1\n",
    "                else: \n",
    "                    idfs[word] = 1\n",
    " \n",
    "    # 计算每个词逆文档值\n",
    "    for word in idfs:\n",
    "        idfs[word] = math.log(d/float(idfs[word]))\n",
    " \n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in sortedidlist:\n",
    "    corpus.append(cleaned_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "idfs = idf(corpus)\n",
    "for doc in corpus:\n",
    "    tfidfs = {}\n",
    "    for word in doc.split():\n",
    "        if word in tfidfs:    \n",
    "            tfidfs[word] += 1\n",
    "        else:\n",
    "            tfidfs[word] = 1\n",
    " #   for word in tfidfs:\n",
    "  #      tfidfs[word] *= idfs[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"cat.csv\")\n",
    "articles = []\n",
    "labels = []\n",
    "for index, row in data.iterrows():\n",
    "    labels.append(row[0])\n",
    "    article = row[1]\n",
    "    for word in STOPWORDS:\n",
    "        token = ' ' + word + ' '\n",
    "        article = article.replace(token, ' ')\n",
    "        article = article.replace(' ', ' ')\n",
    "    articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(articles) * training_portion)\n",
    "\n",
    "train_articles = articles[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_labels = labels[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word=label_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 394,694\n",
      "Trainable params: 394,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "model.load_weights('classify.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence):\n",
    "    txt = [sentence]\n",
    "    seq = tokenizer.texts_to_sequences(txt)\n",
    "    padded = pad_sequences(seq, maxlen=max_length)\n",
    "    pred = model.predict(padded)\n",
    "    label_index_word={1: 'sport', 2: 'business', 3: 'politics', 4: 'tech', 5: 'life'}\n",
    "    return(label_index_word[np.argmax(pred)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
